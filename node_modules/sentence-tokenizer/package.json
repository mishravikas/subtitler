{
  "_args": [
    [
      {
        "raw": "sentence-tokenizer@^0.0.8",
        "scope": null,
        "escapedName": "sentence-tokenizer",
        "name": "sentence-tokenizer",
        "rawSpec": "^0.0.8",
        "spec": ">=0.0.8 <0.0.9",
        "type": "range"
      },
      "/Users/vikasmishra/dev/absentia/subtitler"
    ]
  ],
  "_from": "sentence-tokenizer@>=0.0.8 <0.0.9",
  "_id": "sentence-tokenizer@0.0.8",
  "_inCache": true,
  "_location": "/sentence-tokenizer",
  "_nodeVersion": "6.9.3",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/sentence-tokenizer-0.0.8.tgz_1495539939230_0.41590318572707474"
  },
  "_npmUser": {
    "name": "parmentf",
    "email": "francois.parmentier@gmail.com"
  },
  "_npmVersion": "3.10.10",
  "_phantomChildren": {},
  "_requested": {
    "raw": "sentence-tokenizer@^0.0.8",
    "scope": null,
    "escapedName": "sentence-tokenizer",
    "name": "sentence-tokenizer",
    "rawSpec": "^0.0.8",
    "spec": ">=0.0.8 <0.0.9",
    "type": "range"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/sentence-tokenizer/-/sentence-tokenizer-0.0.8.tgz",
  "_shasum": "6eb5d44aabc58bf579a2a28549ddf1c553c1c7e8",
  "_shrinkwrap": null,
  "_spec": "sentence-tokenizer@^0.0.8",
  "_where": "/Users/vikasmishra/dev/absentia/subtitler",
  "author": {
    "name": "FranÃ§ois Parmentier"
  },
  "bugs": {
    "url": "https://github.com/parmentf/node-sentence-tokenizer/issues"
  },
  "dependencies": {
    "debug": "*",
    "sugar": "1.3.x"
  },
  "description": "Tokenize paragraphs into sentences, and smaller tokens.",
  "devDependencies": {
    "mocha": "1.7.x"
  },
  "directories": {},
  "dist": {
    "shasum": "6eb5d44aabc58bf579a2a28549ddf1c553c1c7e8",
    "tarball": "https://registry.npmjs.org/sentence-tokenizer/-/sentence-tokenizer-0.0.8.tgz"
  },
  "gitHead": "21416ccabdef04645bb482291687ceaf7d173537",
  "homepage": "http://github.com/parmentf/node-sentence-tokenizer",
  "keywords": [
    "tokenizer",
    "sentence"
  ],
  "license": "MIT",
  "main": "lib/tokenizer.js",
  "maintainers": [
    {
      "name": "parmentf",
      "email": "francois.parmentier@gmail.com"
    }
  ],
  "name": "sentence-tokenizer",
  "optionalDependencies": {},
  "readme": "ERROR: No README data found!",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/parmentf/node-sentence-tokenizer.git"
  },
  "scripts": {
    "test": "make test"
  },
  "version": "0.0.8"
}
